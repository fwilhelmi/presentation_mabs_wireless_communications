<!doctype html>
<html>
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<title>reveal.js</title>

		<link rel="stylesheet" href="css/reveal.css">
		<link rel="stylesheet" href="css/theme/black.css">

		<!-- Theme used for syntax highlighting of code -->
		<link rel="stylesheet" href="lib/css/zenburn.css">

		<!-- Printing and PDF exports -->
		<script>
			var link = document.createElement( 'link' );
			link.rel = 'stylesheet';
			link.type = 'text/css';
			link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
			document.getElementsByTagName( 'head' )[0].appendChild( link );
		</script>
	</head>
	<body>
		<div class="reveal">
			<div class="slides">
				<!-- Front page -->
				<section data-background="#ffffff">
					<h2>An introduction to Multi-Armed Bandits for Wireless Communications</h2>
					<h3>Francesc Wilhelmi</h3>
						<img src="assets/upf_logo.png">
					<p>Full presentation available <a href="https://github.com/fwilhelmi/presentation_mabs_wireless_communications/blob/master/intro_mabs_wireless_communications.pptx" target="_blank">here</a></p>
				</section>

				<!-- Part 1 -->
				<section data-background="#ffffff">
					<h1>PART I</h1>
					<h2>Bandits Theory</h2>
				</section>

				<section data-background="#ffffff">
					<ul>
					  <li>The Multi-Armed Bandits (MABs) problem addresses the exploration-exploitation trade-off in face of <b>uncertainty</b></li>
					  <li>Classically, a gambler attempts to maximize the profits among a set of slot machines</li>
					  <li>Often, we refer to the <b>expected cumulative regret</b> to quantify the performance of a given action-selection strategy</li>
					  <ul>
					  	<li>Take the oracle’s policy as a reference</li>
					  	<li>Convergence analysis: anytime, fixed-horizon or asymptotic optimality</li>
					  </ul>
					</ul>
				</section>

				<section data-background="#ffffff">
					<ul>
					  <li>MABs are a class of sequential learning framework that allow to address the tradeoff between <b>exploration</b> and <b>exploitation</b> in face of <b>uncertainty</b></li>
					  <li>Suits problems with unavailable or incomplete information</li>
					  <li>There’s no need of modeling <b>states</b> or trying to learn them (which entails added complexity)</li>
					  <li>Many different MABs-based models have been provided for several types of problems</li>
					</ul>
				</section>

				<section data-background="#ffffff">
					<ul>
					  <li>Baseline:</li>
						<ul>
						  <li style="font-size:26px">Katehakis, M. N., & Veinott Jr, A. F. (1987). The multi-armed bandit problem: decomposition and computation. Mathematics of Operations Research, 12(2), 262-268.</li>
						  <li style="font-size:26px">Gittins, J., Glazebrook, K., & Weber, R. (2011). Multi-armed bandit allocation indices. John Wiley & Sons.</li>
						  <li style="font-size:26px">Auer, P., Cesa-Bianchi, N., & Fischer, P. (2002). Finite-time analysis of the multiarmed bandit problem. Machine learning, 47(2-3), 235-256.</li>
						  <li style="font-size:26px">Auer, P., Cesa-Bianchi, N., Freund, Y., & Schapire, R. E. (1995, October). Gambling in a rigged casino: The adversarial multi-armed bandit problem. In Foundations of Computer Science, 1995. Proceedings., 36th Annual Symposium on (pp. 322-331). IEEE.</li>
						</ul>
					  <li>Overviews:</li>
						<ul>
						  <li style="font-size:26px">Bubeck, S., & Cesa-Bianchi, N. (2012). Regret analysis of stochastic and nonstochastic multi-armed bandit problems. Foundations and Trends® in Machine Learning, 5(1), 1-122.</li>
						  <li style="font-size:26px">Vermorel, J., & Mohri, M. (2005, October). Multi-armed bandit algorithms and empirical evaluation. In European conference on machine learning (pp. 437-448). Springer, Berlin, Heidelberg.</li>
					</ul>
				</section>

				<!-- MABs taxonomy -->
				<section data-background="#ffffff">
					<h1>MABs Taxonomy</h1>
				</section>

				<section data-background="#ffffff">
					<ul>
					  <li>Stochastic</li>
					  <ul>
					  	<li>The reward is generated according to a specific stochastic process</li>
					  	<li>Literature: [5]</li>
					  </ul>
					  <li>Non-stochastic:</li>
					  <ul>
					  	<li>No statistic assumption can be done on the reward generation process</li>
					  	<li>Literature: [5, 7]</li>
					  </ul>
					</ul>
				</section>

				<section data-background="#ffffff">
					<ul>
					  <li>More subtle categorization on reward generating process:</li>
					  <ul>
					  	<li style="font-size:30px"><b>Stateful (Markovian):</b> every arm is associated with some finite state space</li>
					  	<li style="font-size:30px"><b>Stateless:</b> arms do not have specific state</li>
					  	<li style="font-size:30px"><b>Adversarial:</b> different learners attempt to maximize a given reward function</li>
					  </ul>
					  <li>Other types:</li>
					  <ul>
					  	<li style="font-size:30px"><b>Contextual:</b> in addition to the reward, some other useful information is provided to the agent</li>
					  	<li style="font-size:30px"><b>Combinatorial:</b> there is a relationship between arms</li>
					  	<li style="font-size:30px"><b>Mortal:</b> arms are available for a certain amount of time</li>
					  	<li style="font-size:30px">...</li>
					  </ul>
					</ul>
				</section>

				<section data-background="#ffffff">
					<ul>
					  <li>Stateful (Markovian)</li>
					  <ul>
					  	<li style="font-size:30px">Rested (frozen/sleeping): the state of a given arm only changes when pulled</li>
					  	<li style="font-size:30px">Restless: the state of every arm changes regardless of the player’s action</li>
					  </ul>
					  <li>Stateless (Stochastic)</li>
					  <ul>
					  	<li style="font-size:30px">The reward generation process is stochastic (stationary or non-stationary density function), and is independent and identically distributed (IID)</li>
					  </ul>
					  <li>Adversarial</li>
					  <ul>
					  	<li style="font-size:30px">Rewards generation cannot be attributed to any stochastic assumption: Oblivious adversaries / Non-oblivious adversaries (min-max strategies)</li>
					  </ul>
					</ul>
				</section>

				<section data-background="#ffffff">
					<ul>
					  <li style="font-size:35px">Contextual bandits [8-9]:in addition to the reward, some other useful information is provided to the agent</li>
					  <li style="font-size:35px">Combinatorial bandits [10-11]: there is a relationship between arms</li>
					  <li style="font-size:35px">Mortal bandits [12]: arms are available for a certain amount of time</li>
					  <li style="font-size:35px">MABs with multiple plays [13-15]: a player can pick several arms simultaneously</li>
					  <li style="font-size:35px">Pure exploration [16-17]: gain the most knowledge over t by exploring, in order to maximize the likelihood of picking the best arm in t+1</li>
					  <li style="font-size:35px">Dueling bandits [18]: the player chooses a pair of arms in each round and observe only their performances (duel between arms)</li>
					</ul>
				</section>

				<!-- Algorithms -->
				<section data-background="#ffffff">
					<h1>Well-known Algorithms</h1>
				</section>

				<section data-background="#ffffff">
					<p><b>Greedy learning</b></p>
					<ul>
					  <li>&epsilon;-greedy [29]</li>
					  <ul>
					  	<li>With probability &epsilon;, choose x<sub>t</sub> ~ Unif(X), </li>
					  	<li>Otherwise, choose x<sub>t</sub> &isin; argmax<sub>x</sub> f&#770;<sub>t</sub></li>
					  </ul>
					  <li>Explores inefficiently</li>
					  <li>With fixed &epsilon;, the average regret grows linearly</li>
					  <li>Time-dependent exploration rate favors efficiency: &epsilon;<sub>t</sub> = &epsilon;<sub>0</sub>/t</li>
					</ul>
				</section>
			
				<section data-background="#ffffff">
					<p><b>Optimistic learning</b></p>
						<div style="width: 100%;">
						   <div style="float:left; width: 50%">
						   	  <ul>
							    <li style="font-size:35px">Upper Confidence Bound (UCB) [3, 28]</li>
								<li style="font-size:35px">Assumes certain distributions for the rewards and picks the optimal arm accordingly</li>
								<li style="font-size:35px">The action-selection process not only takes the magnitude of the reward into account, but the estimated deviations</li>
							  </ul>
						   </div>
						   <div style="float:right;">
						   	 <img src="assets/ucb_example.png" width="450" height="250" align="middle">
						   </div>
						</div>
						<div style="clear:both"></div>
				</section>

				<section data-background="#ffffff">
					<p><b>Probability Matching learning</b></p>
					<ul>
					  <li>Thompson sampling [30]</li>
				  	  <li>Has been shown to achieve strong performance guarantees, often better than those warranted by UCB [31, 32, 33]</li>
				  	  <li>It constructs a probabilistic model of the rewards and assumes a prior distribution of the parameters of said model – Then, it chooses the arm matching the probability of being optimal</li>
				  	  <li>Very popular because it is computationally cheap and offers good results</li>
					</ul>
				</section>

				<!-- Single-Agent vs Multi-Agent MABs -->
				<section data-background="#ffffff">
					<h1>Single-Agent vs Multi-Agent MABs</h1>
				</section>

				<section data-background="#ffffff">
					<ul>
					  <li>SA-MAB: mainly concerned on sequential online decision making problem in an unknown environment</li>
				  	  <li>MA-MAB: where Game Theory and MABs meet and complement each other (competition between agents)</li>
					</ul>
					<img src="assets/ma_mabs.png" width="450" height="250" align="middle">
				</section>		

				<section data-background="#ffffff">
					<p><b>Single-Agent</b></p>
					<ul>
					  <li>Baseline: [19, 20]</li>
				  	  <li>Properties:</li>
				  	  	<ul>
						  <li>Stationarity of the environment</li>
					  	  <li>Pure exploration-exploitation</li>
					  	  <li>The Regret measurement is completely meaningful in this setting</li>
						</ul>
					  <li>State-of-the-Art performance bounds:</li>
				  	  	<ul>
						  <li>Lai and Robbins, 1985</li>
					  	  <li>Anantharam et al., 1987 (multiple plays)</li>
						</ul>
					</ul>
				</section>	

				<section data-background="#ffffff">
					<p><b>Multi-Agent</b></p>
					<ul>
				  	  <li>Properties:</li>
				  	  	<ul>
						  <li>Non-stationarity of the environment</li>
					  	  <li>Nexus with Game Theory</li>
					  	  <li>Adversarial MABs is a sub-class of the Multi-Agent MAB problem</li>
						</ul>
					  <li>Models:</li>
				  	  	<ul>
						  <li>Decentralized – Selfish behavior</li>
					  	  <li>Distributed – Communication among agents</li>
					  	  <li>Centralized – Correlated Equilibria</li>
						</ul>
					</ul>
				</section>	
					
				<!-- Adversarial MABs -->
				<section data-background="#ffffff">
					<h1>Adversarial MABs</h1>
				</section>	

			</div>
		</div>

		<script src="lib/js/head.min.js"></script>
		<script src="js/reveal.js"></script>

		<script>
			// More info about config & dependencies:
			// - https://github.com/hakimel/reveal.js#configuration
			// - https://github.com/hakimel/reveal.js#dependencies
			Reveal.initialize({
				dependencies: [
					{ src: 'plugin/markdown/marked.js' },
					{ src: 'plugin/markdown/markdown.js' },
					{ src: 'plugin/notes/notes.js', async: true },
					{ src: 'plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } }
				]
			});
		</script>
	</body>
</html>
