<!doctype html>
<html>
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<title>reveal.js</title>

		<link rel="stylesheet" href="css/reveal.css">
		<link rel="stylesheet" href="css/theme/black.css">

		<!-- Theme used for syntax highlighting of code -->
		<link rel="stylesheet" href="lib/css/zenburn.css">

		<!-- Printing and PDF exports -->
		<script>
			var link = document.createElement( 'link' );
			link.rel = 'stylesheet';
			link.type = 'text/css';
			link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
			document.getElementsByTagName( 'head' )[0].appendChild( link );
		</script>
	</head>
	<body>
		<div class="reveal">
			<div class="slides">
				<!-- Front page -->
				<section data-background="#ffffff">
					<h2>An introduction to Multi-Armed Bandits for Wireless Communications</h2>
					<h3>Francesc Wilhelmi</h3>
						<img src="assets/upf_logo.png">
					<p>Full presentation available <a href="https://github.com/fwilhelmi/presentation_mabs_wireless_communications/blob/master/intro_mabs_wireless_communications.pptx" target="_blank">here</a></p>
				</section>

				<!-- Part 1 -->
				<section data-background="#ffffff">
					<h1>PART I</h1>
					<h2>Bandits Theory</h2>
				</section>

				<section data-background="#ffffff">
					<ul>
					  <li>The Multi-Armed Bandits (MABs) problem addresses the exploration-exploitation trade-off in face of <b>uncertainty</b></li>
					  <li>Classically, a gambler attempts to maximize the profits among a set of slot machines</li>
					  <li>Often, we refer to the <b>expected cumulative regret</b> to quantify the performance of a given action-selection strategy</li>
					  <ul>
					  	<li>Take the oracle’s policy as a reference</li>
					  	<li>Convergence analysis: anytime, fixed-horizon or asymptotic optimality</li>
					  </ul>
					</ul>
				</section>

				<section data-background="#ffffff">
					<ul>
					  <li>MABs are a class of sequential learning framework that allow to address the tradeoff between <b>exploration</b> and <b>exploitation</b> in face of <b>uncertainty</b></li>
					  <li>Suits problems with unavailable or incomplete information</li>
					  <li>There’s no need of modeling <b>states</b> or trying to learn them (which entails added complexity)</li>
					  <li>Many different MABs-based models have been provided for several types of problems</li>
					</ul>
				</section>

				<section data-background="#ffffff">
					<ul>
					  <li>Baseline:</li>
						<ul>
						  <li style="font-size:20px">Katehakis, M. N., & Veinott Jr, A. F. (1987). The multi-armed bandit problem: decomposition and computation. Mathematics of Operations Research, 12(2), 262-268.</li>
						  <li style="font-size:20px">Gittins, J., Glazebrook, K., & Weber, R. (2011). Multi-armed bandit allocation indices. John Wiley & Sons.</li>
						  <li style="font-size:20px">Auer, P., Cesa-Bianchi, N., & Fischer, P. (2002). Finite-time analysis of the multiarmed bandit problem. Machine learning, 47(2-3), 235-256.</li>
						  <li style="font-size:20px">Auer, P., Cesa-Bianchi, N., Freund, Y., & Schapire, R. E. (1995, October). Gambling in a rigged casino: The adversarial multi-armed bandit problem. In Foundations of Computer Science, 1995. Proceedings., 36th Annual Symposium on (pp. 322-331). IEEE.</li>
						</ul>
					  <li>Overviews:</li>
						<ul>
						  <li style="font-size:20px">Bubeck, S., & Cesa-Bianchi, N. (2012). Regret analysis of stochastic and nonstochastic multi-armed bandit problems. Foundations and Trends® in Machine Learning, 5(1), 1-122.</li>
						  <li style="font-size:20px">Vermorel, J., & Mohri, M. (2005, October). Multi-armed bandit algorithms and empirical evaluation. In European conference on machine learning (pp. 437-448). Springer, Berlin, Heidelberg.</li>
					</ul>
				</section>

				<!-- MABs taxonomy -->
				<section data-background="#ffffff">
					<h1>MABs Taxonomy</h1>
				</section>

				<section data-background="#ffffff">
					<ul>
					  <li>Stochastic</li>
					  <ul>
					  	<li>The reward is generated according to a specific stochastic process</li>
					  	<li>Literature: [5]</li>
					  </ul>
					  <li>Non-stochastic:</li>
					  <ul>
					  	<li>No statistic assumption can be done on the reward generation process</li>
					  	<li>Literature: [5, 7]</li>
					  </ul>
					</ul>
				</section>

				<section data-background="#ffffff">
					<ul>
					  <li>More subtle categorization on reward generating process:</li>
					  <ul>
					  	<li style="font-size:20px"><b>Stateful (Markovian):</b> every arm is associated with some finite state space</li>
					  	<li style="font-size:20px"><b>Stateless:</b> arms do not have specific state</li>
					  	<li style="font-size:20px"><b>Adversarial:</b> different learners attempt to maximize a given reward function</li>
					  </ul>
					  <li>Other types:</li>
					  <ul>
					  	<li style="font-size:20px"><b>Contextual:</b> in addition to the reward, some other useful information is provided to the agent</li>
					  	<li style="font-size:20px"><b>Combinatorial:</b> there is a relationship between arms</li>
					  	<li style="font-size:20px"><b>Mortal:</b> arms are available for a certain amount of time</li>
					  	<li style="font-size:20px">...</li>
					  </ul>
					</ul>
				</section>

				<section data-background="#ffffff">
					<ul>
					  <li>Stateful (Markovian)</li>
					  <ul>
					  	<li style="font-size:20px">Rested (frozen/sleeping): the state of a given arm only changes when pulled</li>
					  	<li style="font-size:20px">Restless: the state of every arm changes regardless of the player’s action</li>
					  </ul>
					  <li>Stateless (Stochastic)</li>
					  <ul>
					  	<li style="font-size:20px">The reward generation process is stochastic (stationary or non-stationary density function), and is independent and identically distributed (IID)</li>
					  </ul>
					  <li>Adversarial</li>
					  <ul>
					  	<li style="font-size:20px">Rewards generation cannot be attributed to any stochastic assumption: Oblivious adversaries / Non-oblivious adversaries (min-max strategies)</li>
					  </ul>
					</ul>
				</section>

				<section data-background="#ffffff">
					<ul>
					  <li style="font-size:25px">Contextual bandits [8-9]:in addition to the reward, some other useful information is provided to the agent</li>
					  <li style="font-size:25px">Combinatorial bandits [10-11]: there is a relationship between arms</li>
					  <li style="font-size:25px">Mortal bandits [12]: arms are available for a certain amount of time</li>
					  <li style="font-size:25px">MABs with multiple plays [13-15]: a player can pick several arms simultaneously</li>
					  <li style="font-size:25px">Pure exploration [16-17]: gain the most knowledge over t by exploring, in order to maximize the likelihood of picking the best arm in t+1</li>
					  <li style="font-size:25px">Dueling bandits [18]: the player chooses a pair of arms in each round and observe only their performances (duel between arms)</li>
					</ul>
				</section>

				<!-- Algorithms -->
				<section data-background="#ffffff">
					<h1>Well-known Algorithms</h1>
				</section>
			

			</div>
		</div>

		<script src="lib/js/head.min.js"></script>
		<script src="js/reveal.js"></script>

		<script>
			// More info about config & dependencies:
			// - https://github.com/hakimel/reveal.js#configuration
			// - https://github.com/hakimel/reveal.js#dependencies
			Reveal.initialize({
				dependencies: [
					{ src: 'plugin/markdown/marked.js' },
					{ src: 'plugin/markdown/markdown.js' },
					{ src: 'plugin/notes/notes.js', async: true },
					{ src: 'plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } }
				]
			});
		</script>
	</body>
</html>
